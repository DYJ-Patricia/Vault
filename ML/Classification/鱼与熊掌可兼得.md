![[Pasted image 20251014000947.png]]

可兼得：
- H很小，candidates少
- H都是精英，都能使L很小

# 前置知识(Hidden layer)

- 用==piecewise linear==来逼近任何一条线，切得越碎越近
- 把piecewise linear分成几个hard sigmoid
## ==Hard Sigmoid==的一种表示方法
- ![[Pasted image 20251014103623.png]]

- ![[Pasted image 20251014103659.png]]

- ![[Pasted image 20251014112004.png]]
##  ==Hard Simoid==的另一种表示方法

- 两个ReLU叠起来


- ![[Pasted image 20251014112307.png]]
- 多个ReLU组成piecewise linear
- ![[Pasted image 20251014112636.png]]
## Deeper is Better?

- H越大，错误率越小，理想越高
- 训练集的资料越多，现实离理想越近。训练资料少容易overfitting
- 同样大小模型情况下，deeper比fatter更有效
- fatter比deeper需要参数更多，故deep learning的优势是参数更少不容易overfitting
- 