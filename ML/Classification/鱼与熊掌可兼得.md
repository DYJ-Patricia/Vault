![[Pasted image 20251014000947.png]]

可兼得：
- H很小，candidates少
- H都是精英，都能使L很小

# 前置知识(Hidden layer)

- 用==piecewise linear==来逼近任何一条线，切得越碎越近
- 把piecewise linear分成几个hard sigmoid
## ==Hard Sigmoid==的一种表示方法
- ![[Pasted image 20251014103623.png]]

- ![[Pasted image 20251014103659.png]]

- ![[Pasted image 20251014112004.png]]
##  ==Hard Simoid==的另一种表示方法

- 两个ReLU叠起来


- ![[Pasted image 20251014112307.png]]
- 多个ReLU组成piecewise linear
- ![[Pasted image 20251014112636.png]]
# Deeper is Better?

- H越大，错误率越小，理想越高
- 训练集的资料越多，现实离理想越近。训练资料少容易overfitting
- 同样大小模型情况下，deeper比fatter更有效
- fatter比deeper需要参数更多，故deep learning的优势是参数更少不容易overfitting

### 推演多层activation function叠加(ReLU)输出结果

- ![[Pasted image 20251014115500.png]]

- 6
- ![[Pasted image 20251014115955.png]]
- ==让loss最低的function如果complex 且 regular 如speech,image,则deep network比shallow network更有优势==

# 结论

==深度学习是可以让鱼与熊掌可兼得的方法，可以在较小的H下得到较小loss==
