- 有些模型内在地具有可解释性，如Linear Model 从weight中可以知道the importance of features,但并不powerful
- Deep network 很强大，但是相当于黑箱，难以解释

# Goal of Explainable ML

- 让人信服


# 分类
共分为两类：Local Explanation,Global Explanation

![[Pasted image 20251024174516.png]]

# Local Explanation(Explain the dicision)

## Which component is critical?

### 最简单的方法（拿方块分别遮挡不同地方）
![[Pasted image 20251024175451.png]]
### Saliency Map

![[Pasted image 20251024175626.png]]
- 先算所有pixel有关的loss
- 选一个pixel做出改动后，再计算loss
- loss变动越大，代表该区域越重要

### Limitation
#### 怎么看得更爽(除去杂讯)
![[Pasted image 20251024180420.png]]
#### Gradient Saturation

- 光看偏微分的结果可能并不能告诉我们一个部位的重要性
- ![[Pasted image 20251024180809.png]]

## Network是怎么处理输入的信息的

### Visualization(肉眼观察)

#### neuron
![[Pasted image 20251024181007.png]]
- 上图是直接拿neuron的输出来进行分析
- 假设输出是100维，那么使用某些方法降到二维然后画到图上

#### Attention

![[Pasted image 20251024181457.png]]
### Probing（用探针观察，相当于classifier,但不一定是classifier）

![[Pasted image 20251024181736.png]]

# Global Explanation

### What does a filter detect?

![[Pasted image 20251024182524.png]]
- 如上图，对于CNN来说，输入image X,一个filter对应输出一个matrix即feature map
- 假设filter1输出的map里有几个数值颇大，则代表image X 包含了filter可以这侧出的patterns
- 但是，global explanation里不会给它提供image，那么得自己创造一张包括了patterns的image，其不在图库里

### 那怎么生成呢？

- 找一个x可以使matrix里的数值最大化
- ![[Pasted image 20251024183514.png]]
- 那个x就是X* 
## What does a digit look like for CNN?
- 接下来，如果做手写的数字辨识，把第二层convolutional layer的所有filters拿出来，找出它们的X*
- ![[Pasted image 20251024183912.png]]
- ![[Pasted image 20251024184401.png]]
- 需要加constraint使其清晰
- 但是太难了，可以用generator来优化，如下图
- ![[Pasted image 20251024184810.png]]


![[Pasted image 20251024185218.png]]


