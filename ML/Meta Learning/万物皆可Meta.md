# Learning to Initialize

## Model-Agnostic Meta-Learning(MAML)
- learn 初始化参数

### Reference
![[Pasted image 20251108195847.png]]

### MAML VS Pre-training
![[Pasted image 20251108202739.png]]
#### self-supervised learning提出之前
![[Pasted image 20251108202646.png]]
- 在提出之前，pre-training也可以认为是multi-task learning
- 所以multi-task learning一般当作Meta的baseline
- 那Meta-learning不就相当于domain adaptation?
- ![[Pasted image 20251108203057.png]]

#### MAML为什么好？

