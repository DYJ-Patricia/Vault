# Background:Pre-trained Language Models(PLMs)
## Neural Language Model

- 预测句子出现的概率
- 可以把language model的训练大概分为两种
- We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks

### Autoregressive Language Models(ALMs)

- Complete the sentence given its prefix
- 如GPT系列

#### Transformer-based ALMs

- Embedding Layer:把离散的token变成连续的向量
- LM(Language Model) Head:Linear Layer
- Language Model的训练就是一种self-supervised

- 把要预测的token之后的内容，就把该token代表的embedding输入LM Head

### Masked Language Models(MLMs)

- Use the unmasked words to predict the masked word
- 如BERT

### Fine-tune/微调

![[Pasted image 20251020221321.png]]

- 做sentiment analysis时在后面加一个classifier head
- fine-tune时不用LM Head
- Embedding和Transformer Layer的参数是从pre-trained 得来的

# The Problem of PLMs

- Problem 1: Data scarcity in downstream tasks - A large amount of labeled data is not easy to obtain for each downstream task
- Problem 2: The PLM is too big, and they are still getting bigger  -Need a copy for each downstream task

# The Solutions of Those Problems

## Data-Efficient Fine-tuning
