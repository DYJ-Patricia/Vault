# Background:Pre-trained Language Models(PLMs)
## Neural Language Model

- 预测句子出现的概率,输出是probability distribution
- 可以把language model的训练大概分为两种
- We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks

### Autoregressive Language Models(ALMs)

- Complete the sentence given its prefix
- 如GPT系列

#### Transformer-based ALMs

- Embedding Layer:把离散的token变成连续的向量
- LM(Language Model) Head:Linear Layer
- Language Model的训练就是一种self-supervised

- 把要预测的token之后的内容，就把该token代表的embedding输入LM Head

### Masked Language Models(MLMs)

- Use the unmasked words to predict the masked word
- 如BERT

### Fine-tune/微调

![[Pasted image 20251020221321.png]]

- 做sentiment analysis时在后面加一个classifier head
- fine-tune时不用LM Head
- Embedding和Transformer Layer的参数是从pre-trained 得来的

# The Problem of PLMs

- Problem 1: Data scarcity in downstream tasks - A large amount of labeled data is not easy to obtain for each downstream task
- Problem 2: The PLM is too big, and they are still getting bigger  -Need a copy for each downstream task

# The Solutions of Those Problems

## Data-Efficient Fine-tuning:Prompt Tuning

- 在做language inference时

- Format the downstream task as a language modelling task with pre-defined templates into natural language prompts
![[Pasted image 20251020224136.png]]

- What you need in prompt tuning:

1. A prompt template: convert data points into a natural language prompt(把premise和hypothesis关联起来)
2. a PLM
 ![[Pasted image 20251020224800.png]]
3. A verbalizer: A mapping between the label and the vocabulary - Which vocabulary should represents the class "entailment"
### 区别
![[Pasted image 20251020225108.png]]
data多的时候，两者表现没什么区别
表现更好的原因：
- Prompt tuning has better performance under data scarcity because 
- It incorporates human knowledge 
-  It introduces no new parameters

### Few-shot Learning
![[Pasted image 20251020225900.png]]
- Few-shot Learning时可以用prompt+demonstration


### Semi-Supervised learning

- Semi-Supervised learning: We have some labeled training data and a large amount of unlabeled data

####  Pattern-Exploiting Training (PET) 
- Step 1: Use different prompts and verbalizer to prompt-tune different PLMs on the labeled dataset
- Step 2: Predict the unlabeled dataset and combine the predictions from different models
-  Step 3: Use a PLM with classifier head to train on the soft-labeled data set

### Zero-shot Learning

- Where does this zero-shot ability spring from? 
- Hypothesis: during pre-training, the training datasets implicitly contains a mixture of different tasks
## Reducing the Number of Parameters

- 减少参数
- share parameters

### Parameter-Efficient Fine-tuning

![[Pasted image 20251021124037.png]]
- 每个downstream task用少量参数
- ![[Pasted image 20251021124347.png]]
- ![[Pasted image 20251021124413.png]]
- fine-tuning就是改变之前pre-train后的hidden representation(h)
- standard fine-tuning要改变所有参数

#### Adapter
- 一种方法，在Transformer的某些地方插入submodule
- Adapters: During fine-tuning, only update the adpaters and the classifier head
- ![[Pasted image 20251021125049.png]]
- ![[Pasted image 20251021125103.png]]

#### LoRA

- 一种方法，在Transformer的某些地方插入submodule,在Feed-forward插入
- ![[Pasted image 20251021125612.png]]
- ![[Pasted image 20251021125733.png]]
- 中间没有Nonlinearity
- During fine-tuning, only update the LoRA and the classifier head
- 本质上和Adapter类似
- 但是Adapter加入后会使结构更深，inference时间更多

#### Prefix Tuning

- 在Transformer layer的self-attention层里加入prefix的module
- ![[Pasted image 20251021130730.png]]
- 训练的时候是上图黄色那团
- 真正pre-fine时可以把蓝色的丢弃，如下图
- ![[Pasted image 20251021130932.png]]

#### Soft Prompting

- 可以看作Prefix Tuning简化版
- 只训练更新黄色就好了
- ![[Pasted image 20251021131226.png]]
- ![[Pasted image 20251021131340.png]]
- hard prompt需要整个都调

#### 好处

![[Pasted image 20251021131707.png]]
![[Pasted image 20251021131913.png]]


### Early Exit

- 动态减少
- ![[Pasted image 20251021132308.png]]
- Reduce the models that are involved during inference

