- Attention matrix可能过大导致计算量大
- 当sequence input 的N非常长时，self-attention dominates computation

# 变型

## Loc