# 原理
- 做分类问题时用
- 把输出的数据做处理，输出的所有概率和为1且非负
- 真实值本来就是one-hot的，所以也相当于概率和为1
![[Pasted image 20251122164000.png]]

# Cross-entropy(交叉熵)
- 有了两个概率，用其计算loss
- 真实类别只有一个概率唯一，所以y_i直接为1

- ![[Pasted image 20251122164108.png]]
- 